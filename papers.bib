@inproceedings{netdissect2017,
  title={Network Dissection: Quantifying Interpretability of Deep Visual Representations},
  author={Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  booktitle={Computer Vision and Pattern Recognition},
  year={2017}
}

@inproceedings{bau2019gandissect,
 title={GAN Dissection: Visualizing and Understanding Generative Adversarial Networks},
 author={Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Zhou, Bolei and Tenenbaum, Joshua B. and Freeman, William T. and Torralba, Antonio},
 booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
 year={2019}
}

@article{DBLP:journals/corr/abs-1807-10221,
  author    = {Tete Xiao and
               Yingcheng Liu and
               Bolei Zhou and
               Yuning Jiang and
               Jian Sun},
  title     = {Unified Perceptual Parsing for Scene Understanding},
  journal   = {CoRR},
  volume    = {abs/1807.10221},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.10221},
  archivePrefix = {arXiv},
  eprint    = {1807.10221},
  timestamp = {Fri, 22 Mar 2019 10:44:25 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-10221},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/NguyenDYBC16,
  author    = {Anh Mai Nguyen and
               Alexey Dosovitskiy and
               Jason Yosinski and
               Thomas Brox and
               Jeff Clune},
  title     = {Synthesizing the preferred inputs for neurons in neural networks via
               deep generator networks},
  journal   = {CoRR},
  volume    = {abs/1605.09304},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.09304},
  archivePrefix = {arXiv},
  eprint    = {1605.09304},
  timestamp = {Mon, 13 Aug 2018 16:46:53 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/NguyenDYBC16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1903-07291,
  author    = {Taesung Park and
               Ming{-}Yu Liu and
               Ting{-}Chun Wang and
               Jun{-}Yan Zhu},
  title     = {Semantic Image Synthesis with Spatially-Adaptive Normalization},
  journal   = {CoRR},
  volume    = {abs/1903.07291},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.07291},
  archivePrefix = {arXiv},
  eprint    = {1903.07291},
  timestamp = {Mon, 01 Apr 2019 14:07:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-07291},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v97-zhang19d,
  title = 	 {Self-Attention Generative Adversarial Networks},
  author = 	 {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7354--7363},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf},
  url = 	 {http://proceedings.mlr.press/v97/zhang19d.html},
  abstract = 	 {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN performs better than prior work, boosting the best published Inception score from 36.8 to 52.52 and reducing Fr√©chet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.}
}

@article{DBLP:journals/corr/abs-1904-10509,
  author    = {Rewon Child and
               Scott Gray and
               Alec Radford and
               Ilya Sutskever},
  title     = {Generating Long Sequences with Sparse Transformers},
  journal   = {CoRR},
  volume    = {abs/1904.10509},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.10509},
  archivePrefix = {arXiv},
  eprint    = {1904.10509},
  timestamp = {Thu, 02 May 2019 15:13:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-10509},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}